{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/elliekim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "Create a variable, **text1**, that stores the user's pitch. \n",
    "NLTK will tokenize this into characters, so we must use **word_tokenize** to obtain tokens as words. \n",
    "\n",
    "Create a variable, **text**, that stores user's pitch with word tokens\n",
    "`text = word_tokenize(text)`\n",
    "\n",
    "Then, for future functions, work with text, instead of text1. This ensures raw data is not manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', \"'m\", 'Ellie', ',', 'a', 'dedicated', 'and', 'ambitious', 'Sophomore', 'at', 'UNC', 'pursuing', 'a', 'major', 'in', 'Computer', 'Science', 'and', 'a', 'double', 'minor', 'in', 'Statistics', 'and', 'Journalism', '.', 'With', 'a', 'passion', 'for', 'technology', ',', 'I', 'am', 'continuously', 'seeking', 'opportunity', 'to', 'create', 'impactful', 'solution', 'and', 'intersect', 'design', 'and', 'technology', '.', 'My', 'drive', 'extends', 'beyond', 'technical', 'skill', ';', 'I', 'am', 'committed', 'to', 'leveraging', 'technology', 'for', 'social', 'good', ',', 'advocating', 'for', 'accessibility', 'and', 'inclusivity', 'in', 'every', 'tech', 'product', 'I', 'work', 'on', '.', 'Connect', 'and', 'collaborate', 'with', 'me', 'at', 'cellie', '@', 'unc.edu', 'to', 'make', 'a', 'positive', 'difference', 'together', '!']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello! I'm Ellie, a dedicated and ambitious Sophomore at UNC pursuing a major in Computer Science and a double minor in Statistics and Journalism. With a passion for technology, I am continuously seeking opportunities to create impactful solutions and intersect design and technology. My drive extends beyond technical skills; I am committed to leveraging technology for social good, advocating for accessibility and inclusivity in every tech product I work on. Connect and collaborate with me at cellie@unc.edu to make a positive difference together!\"\n",
    "\n",
    "def words(text1):\n",
    "    return word_tokenize(text1)\n",
    "\n",
    "text = words(text1)\n",
    "\n",
    "lower_case_text = text1.lower()\n",
    "lemmatizedWords = []\n",
    "for word in text: \n",
    "    rootWord = lemmatizer.lemmatize(word)\n",
    "    lemmatizedWords.append(rootWord)\n",
    "print(lemmatizedWords)\n",
    "\n",
    "def stop_word(word):\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    if word.lower() in stopwords_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Manipulation\n",
    "`sorted(set(text))` returns the tokens in alphabetical order, with capitalized words coming first\n",
    "`len(set(text1))` returns the total # of distinct words in the text, while `len(text` returns the entire length\n",
    "\n",
    "## Calculating the lexical richness, or the number of distinct words out of the total # of words\n",
    "Type-Token Ratio(TTR): Our ratio calculates the # of unique words in proportion to the total number of words. \n",
    "Generally, a TTR between 0.2 and 0.4 is considered average, while a TTR above 0.4 is often seen as more diverse and rich.\n",
    "`len(set(text1)) / len(text1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text has lexical richness!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7052631578947368"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function calculates the lexical richness, or the number of distinct words out of the total # of words\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    diversity = len(set(text))/len(text)\n",
    "    if (diversity > 0.4):\n",
    "        print(\"Your text has lexical richness!\")\n",
    "    else:\n",
    "        print(\"You have low lexical richness. Try diversifying the words you are using.\")\n",
    "    return diversity\n",
    "\n",
    "lexical_diversity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution\n",
    "Tells us the frequency of each vocabulary item in the text\n",
    "Can determine the most over-used words\n",
    "\n",
    "The function below creates a frequency distribution of words in the text. \n",
    "Then, it returns the words in the text that appears more than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have no overused words in your text. Good job!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def overused_words(text):\n",
    "    fdist1 = FreqDist(text)    \n",
    "    V = set(text)\n",
    "    overused = [w for w in V if fdist1[w] > 4 and not stop_word(w)]\n",
    "    if len(overused) == 0: \n",
    "        print(\"You have no overused words in your text. Good job!\")\n",
    "    else: \n",
    "        print(\"Here are your overused words. Here are some alternatives you can use: \")\n",
    "    return(overused)\n",
    "\n",
    "overused_words(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Pretrained Model\n",
    "Use a mdoel trained on a large corpus of data\n",
    "Transformer model accounts for the words but also the context related to other words\n",
    "\n",
    "**Important note: make sure to run functions on RAW text, since word_tokenized text is an array of string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text is too negative. Try adding some positive words: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Here is your roberta_neg score': 0.9823239,\n",
       " 'Here is your roberta_neu score': 0.01427591,\n",
       " 'Here is your roberta_pos score': 0.0034001828}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(text1, return_tensors = 'pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'Here is your roberta_neg score' : scores[0],\n",
    "        'Here is your roberta_neu score' : scores[1],\n",
    "        'Here is your roberta_pos score' : scores[2]\n",
    "    }\n",
    "    if (scores[0] > 0.2):\n",
    "        print(\"Your text is too negative. Try adding some positive words: \")\n",
    "    return scores_dict\n",
    "\n",
    "polarity_scores_roberta(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Skills from Elevator Pitch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Ellie', 'dedicated', 'ambitious', 'Sophomore', 'UNC', 'major', 'Computer', 'Science', 'double', 'minor', 'Statistics', 'Journalism', 'passion', 'technology', 'opportunities', 'impactful', 'solutions', 'intersect', 'design', 'technology', 'drive', 'extends', 'technical', 'skills', 'technology', 'social', 'good', 'accessibility', 'inclusivity', 'tech', 'product', 'Connect', 'collaborate', 'cellie', '@', 'unc.edu', 'positive', 'difference']\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = pos_tag(text)\n",
    "relevant_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ']\n",
    "skills = [token for token, tag in tagged_tokens if tag in relevant_tags]\n",
    "print(skills)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
